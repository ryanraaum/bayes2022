---
title: "Your First Bayesian Regression"
author: "Ryan Raaum - Spring 2022"
course: "Introduction to Bayesian Statistics"
output:
  rrmdstyle::html_worksheet: default
  rrmdstyle::pdf_worksheet: default
---

## Let's get this started!

This is our first worksheet of the class. If you've taken my Quantitative Methods class, you'll know the drill. But if you took my class before last fall, you've not seen the all-electronic version of my worksheets. And even for those who did take my class last fall, there are some changes as I've worked out some of the kinks.

There's always going to be a `setup` code chunk at the start of the worksheet where you'll load the packages that are needed to complete the worksheet. I'll include some comments the first time or so that we use a package.

```{r setup, message=FALSE}
library(qmdata) # my example datasets package
library(here) # robust paths to files included in the project
library(ggplot2) # plotting
library(patchwork) # for laying out plots
theme_set(theme_minimal()) # not necessary, I just don't like the default ggplot theme
library(rstanarm) # for Bayesian modeling
library(bayesplot) # for plotting of Bayesian models
library(posterior) # manipulate the posterior distribution
library(dplyr) # for data wrangling
```

You will use data from a study of the effects of ivory poaching on the length and circumference of tusks of African elephants in eastern Africa ([Chiyo et al. 2015](https://doi.org/10.1002/ece3.1769)). In addition to tusk `length` and `circumference`, the data include `time` period of collection (`1966-68` before widespread poaching and after during `2005-13`), `age`, `height` at shoulder, `sex`, and an individual `id`.

For the purposes of this worksheet, we will focus on tusk length (response variable) with height, time period, and sex as predictors. 

## Visualize the data

Let's first look at plots tusk length against height colored by sex/time period and verify that a linear model would be appropriate for these data.

```{r exploratory-plots, fig.width=7}
p1 <- ggplot(tusksize, aes(length, height, color = sex)) +
  geom_point() +
  labs(x = "Height (cm)", y = "Length (cm)")

p2 <- ggplot(tusksize, aes(length, height, color = time)) +
  geom_point() +
  labs(x = "Height (cm)", y = "Length (cm)")

p1 + p2
```

That looks fine for linear regression modeling and there is an apparent difference between the sexes and time periods. However, there does look like there is likely to be some heteroscedasticity.

## Some data pre-processing

First, we'll log transform the numeric variables. If the data are all greater than zero, it can never really hurt and in this case it reduces the heteroscedasticity that you can see in the plots above.

```{r some-log-transforms}
tusksize <- tusksize %>% 
  mutate(loglength = log(length),
         logheight = log(height))
```

Next, following the lead of the paper, we'll center (but not scale) the height data. By doing this, the intercept becomes the predicted tusk length of an elephant of average height.

```{r center-logheight}
tusksize <- tusksize %>% 
  mutate(logheight = scale(logheight, center = TRUE, scale = FALSE)[,1])
```

## Ordinary linear regression

Let's first acquaint ourselves with the data through an ordinary least squares linear regression using the base R `lm` function. To start with, we'll include the main terms as well as all the first order interactions. As I did in the slides in lecture, we'll create the formula outside of the modeling function so we can reuse it later.

```{r lm-regression}
tuskformula1 <- loglength ~ logheight + sex + time +
  logheight:sex + logheight:time + sex:time
fit1 <- lm(tuskformula1, data=tusksize)
summary(fit1)
```

So, there's a statistically significant relationship of tusk length with height with a predicted 2.5% increase in tusk length for every 1% increase in height. There are also significant interactions involving the time period. Overall, the model explains about 87% of the variation in tusk length.

## Bayesian linear regression

First, let's create some common settings for the MCMC sampler. 4 chains is typical. For the number of cores, `parallel::detectCores()` will identify the number of cores available on your computer (which should definitely have several) and then we'll save one for other things. Finally, the seed can be anything, but it's generally good practice to have one for reproducibility.

```{r sampler-setup}
CHAINS = 4
CORES = parallel::detectCores() - 1
SEED = 1234
```

Now we can fit a Bayesian regression model. We'll use the `stan_lm` function from `rstanarm` with a `R2` prior. In the function call below, replace `REPLACE` with a good starting value for the likely R-squared of the model. 

```{r rstanarm-regression}
fit2 <- stan_lm(tuskformula1,
                data = tusksize,
                prior = R2(REPLACE, what="mean"),
                chains = CHAINS, cores = CORES, seed = SEED)
```

Hopefully that completed without any divergences on your computer (it did on mine). How would you look at a summary of the model?

```{r}
#
```

Here, the intercept is the expected (log) tusk length for a *female* elephant of average height during the *1966-68* time period. What is that expected length?

```{r}
exp(REPLACE)
```

But we're getting a little ahead of ourselves here. Let's first check some of the MCMC and model diagnostics.

First, we can get an overview of some of the sampler diagnostics using the `check_hmc_diagnostics` function from `rstan`. Note that you need to reference the `stanfit` object inside the model object here.

```{r hmc-diagnostics}
rstan::check_hmc_diagnostics(fit2$stanfit)
```

We saw what divergences are in the lecture. 

"Tree Depth" is part of the No-U-Turn-Sampler algorithm that relates to how efficiently the sampler is performing; if there were some iterations that hit the maximum tree depth it wouldn't be a big deal. But if there were a lot, the sampler isn't performing very efficiently.

Finally, the "Energy" is another measure of how efficiently the sampler is working. Pathological data here would suggest some re-parameterization of your model.

Let's extract the posterior distribution from the model using the `as_draws` function from the `posterior` package and look at the MCMC diagnotics to assess convergence.

Again, note that you need to reference the `stanfit` object within the model object. In this case, the necessity for this should go away at some point, but it's here for now.

```{r assess-convergence}
fit2draws <- as_draws(fit2$stanfit)
summarize_draws(fit2draws, default_convergence_measures())
```

Here, we need the `rhat` values to all be very close to 1.00. They all looked good when I ran it.

There are 4000 samples in the posterior draw, so for the effective sample size measures, we want high hundreds to thousands here. They all looked fine when I ran it. If you saw low hundreds, you should be concerned.

We can visualize the MCMC posterior distribution using the `mcmc_*` functions from `bayesplot`. For now, let's just look at the posterior distributions and traces for a couple parameters using the `mcmc_combo` function.

```{r mcmc-combo}
mcmc_combo(fit2draws, pars=c("(Intercept)", "logheight"))
```

How would you plot the distribution and traces for all the interaction terms?

```{r mcmc-combo-interactions}
mcmc_combo(fit2draws, pars=REPLACE)
```

Ok. So the MCMC sampler seems to have worked well. Now, how does the model fit?

We can look at our usual diagnostics. First, let's plot the residuals against the fitted values.

```{r residuals-vs-fitted}
plot(resid(fit2) ~ fitted(fit2))
```

How does that look?

Let's do a QQ plot.

```{r qq-plot}
qqnorm(resid(fit2)); qqline(resid(fit2))
```

How does that look?

# Posterior Predictive Distribution

Let's generate the posterior predictive distribution and use it to see how well our model captures the features of the observed data.

```{r generate-ppd}
fit2ppd <- posterior_predict(fit2)
```

First, let's see how the samples match the overall observed distribution of tusk length.

```{r density-overlay}
ppc_dens_overlay(tusksize$loglength, fit2ppd[1:50,])
```

This isn't terrible, but it's not amazing either. 

We have two difference categorical terms in the model (`sex` and `time`), so let's plot out the posterior predictive distribution for all combinations of those. First, we'll create a new factor with the combined levels and then we'll use `ppc_dens_overlay_grouped` to plot it out.

```{r density-overlay-grouped}
timebysex <- paste(tusksize$sex, tusksize$time, sep=":")
ppc_dens_overlay_grouped(tusksize$loglength, fit2ppd[1:50,], timebysex)
```

Again, none of these are terrible, but they're not spectacular either. I suspect that the data probably need some curation. For instance, in the 2005-13 time period, there are 16 elephants born in 1970 or earlier - so they do not necessarily represent the effects of poaching in the 70s and 80s. In addition, in both time periods there are a lot of immature elephants (> 50% in both), so some of the heterogeneity in the observed data might be a result of this.

# Model estimates

Finally, we can get our coefficient and related parameter estimates from the draws. (This is available in the model summary as well, but the `summarize_draws` function produces a data frame [tibble] that is more flexible.)

```{r}
summarize_draws(fit2draws, default_summary_measures()) %>% 
  mutate(across(where(is.numeric), ~round(., 3)))
```

